{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ERd3n2rP5dLB",
    "outputId": "a65be561-86a7-44c1-d154-0065c76bfced"
   },
   "outputs": [],
   "source": [
    "!pip install transformers  bokeh jupyter_bokeh umap-learn pandas matplotlib datashader bokeh holoviews scikit-image  colorcet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGVIzNwB-45a"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5de13a25gjs"
   },
   "outputs": [],
   "source": [
    "from bokeh.layouts import grid\n",
    "from bokeh.models import ColumnDataSource \n",
    "from bokeh.models.widgets.tables import DataTable, TableColumn\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.palettes import d3\n",
    "\n",
    "import umap\n",
    "import umap.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLqxb7Q7_TM2"
   },
   "source": [
    "# Compute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wq0-aOO45YtN"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "\n",
    "#Sentences we want sentence embeddings for\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "             'Sentences are passed as a list of string.',\n",
    "             'The quick brown fox jumps over the lazy dog.']*10\n",
    "\n",
    "#Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpBNuLPJ5xwY"
   },
   "outputs": [],
   "source": [
    "sentence_embeddings.shape\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKj5t7nU7mCO"
   },
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3kjZHCY6kIv"
   },
   "outputs": [],
   "source": [
    "# do umap on the embeddings\n",
    "samp_mapper = umap.UMAP(random_state=42).fit(sentence_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ccek7J_V8q-R",
    "outputId": "610ddda6-570d-4143-84f3-5ae4daa5a405"
   },
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "T3P1RaVO9b_n",
    "outputId": "fc5d1375-56ab-43d5-cab2-57f41f57036a"
   },
   "outputs": [],
   "source": [
    "l = len(df)     \n",
    "data = pd.DataFrame(samp_mapper.embedding_[:l], columns=(\"x\", \"y\"))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arxT3ZCY6cit"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "def plot_emb_app(doc):\n",
    "    # CONSTANTS\n",
    "    l = len(samp) \n",
    "    \n",
    "    # lang1, lang2, label_col = 'en', 'ga', 'noise_type'\n",
    "    plot_width,plot_height = 525,525\n",
    "    \n",
    "    # DATA\n",
    "    data = pd.DataFrame(samp_mapper.embedding_[:l], columns=(\"x\", \"y\"))\n",
    "    # df = samp[[lang1,lang2,'main_index', label_col]].iloc[:l,:]\n",
    "    # hover_data = df[[lang1,lang2,label_col,'main_index']]\n",
    "    # data[label_col] = df[label_col]\n",
    "    \n",
    "    # SET LABELS\n",
    "    # labels = samp[label_col]\n",
    "     \n",
    "    # TOOLTIP\n",
    "    # tooltip_dict = {}\n",
    "    # for col_name in hover_data:\n",
    "    #     data[col_name] = hover_data[col_name]\n",
    "    #     tooltip_dict[col_name] = \"@\" + col_name\n",
    "    # tooltips = list(tooltip_dict.items())\n",
    "    \n",
    "    # COLOR BY LABEL\n",
    "    # palette = d3['Category10'][len(df[label_col].unique())]\n",
    "    # colormap = {x:palette[i] for i,x in enumerate(df[label_col].unique())}\n",
    "    # data['colors'] = [colormap[x] for x in df[label_col]]\n",
    "    \n",
    "    # PLOT1\n",
    "    s1 = ColumnDataSource(data=data)\n",
    "    p1 = figure(\n",
    "        # tooltips=tooltips,\n",
    "        plot_width=plot_width,\n",
    "        plot_height=plot_height,\n",
    "        tools=['lasso_select','pan','wheel_zoom','box_zoom','reset','save'],\n",
    "        active_drag=\"lasso_select\",\n",
    "        title=\"Select points with Lasso tool\",\n",
    "    )\n",
    "    p1.title.vertical_align = 'top'\n",
    "    \n",
    "    # # CREATE CIRCLE 1\n",
    "    # c1=p1.circle(\"x\", \"y\", source=s1, alpha=0.5, color='colors', size=2)\n",
    "    # ds1 = c1.data_source\n",
    "    \n",
    "    # # PLOT2\n",
    "    # s2 = ColumnDataSource(data=dict(x=[], y=[], idx=[]))\n",
    "    # p2 = figure(\n",
    "    #     plot_width=int(plot_width/2),\n",
    "    #     plot_height=int(plot_height/2),\n",
    "    #     x_range=(min(data['x'])-1, max(data['x'])+1),\n",
    "    #     y_range=(min(data['y'])-1, max(data['y'])+1),\n",
    "    #     title=\"Selected Points from Main Plot\",\n",
    "    # )\n",
    "    # p2.title.vertical_align = 'top'\n",
    "    # c2 = p2.circle(\"x\", \"y\", source=s2, alpha=0.6, color=\"firebrick\", size=3)\n",
    "    # ds2 = c2.data_source\n",
    "    \n",
    "    # # SELECT CALLBACK\n",
    "    # def select_callback(attrname, old, new):\n",
    "    #     global selected_indxs        \n",
    "    #     d1 = ds1.data\n",
    "    #     d2 = dict()\n",
    "    #     selected_indxs = new\n",
    "    #     xs,ys,t1,t2,idxs,lab1 = [],[],[],[],[],[]\n",
    "        \n",
    "    #     for i in range(len(selected_indxs)):\n",
    "    #         xs.append(d1['x'][selected_indxs[i]])\n",
    "    #         ys.append(d1['y'][selected_indxs[i]])\n",
    "    #         lab1.append(d1[label_col][selected_indxs[i]])\n",
    "    #         t1.append(d1[lang1][selected_indxs[i]])\n",
    "    #         t2.append(d1[lang2][selected_indxs[i]])\n",
    "    #     d2['x'], d2['y'], d2['idx'], d2[lang1], d2[lang2], d2[label_col] = xs, ys, selected_indxs, t1, t2, lab1\n",
    "    #     ds2.data = d2 # BEST PRACTICE --- update .data in one step with a new dict\n",
    "    \n",
    "    # s1.selected.on_change('indices', select_callback)\n",
    "\n",
    "    # # TABLE OF SELECTED POINTS\n",
    "    # columns = [\n",
    "    #     TableColumn(field=\"idx\", title=\"data idx\", width=15),\n",
    "    #     TableColumn(field=lang1, title=lang1),\n",
    "    #     TableColumn(field=lang2, title=lang2),\n",
    "    #     TableColumn(field=label_col, title=label_col, width=50),\n",
    "    #     TableColumn(field=\"x\", title=\"X axis\", width=40),\n",
    "    #     TableColumn(field=\"y\", title=\"Y axis\", width=40),\n",
    "    # ]\n",
    "    # table = DataTable(\n",
    "    #     source=s2,\n",
    "    #     columns=columns,\n",
    "    #     width=900,\n",
    "    #     height=400,\n",
    "    #     sortable=True,\n",
    "    #     selectable=True,\n",
    "    #     editable=True,\n",
    "    # )\n",
    "    \n",
    "    doc.add_root(grid([p1], ncols=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "eCwfPW95Agnt",
    "outputId": "d7ac8213-8099-440d-c300-d94133596361"
   },
   "outputs": [],
   "source": [
    "# Plot a complex chart with interactive hover in a few lines of code\n",
    "\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.sampledata.autompg import autompg_clean as df\n",
    "from bokeh.transform import factor_cmap\n",
    "\n",
    "df.cyl = df.cyl.astype(str)\n",
    "df.yr = df.yr.astype(str)\n",
    "\n",
    "group = df.groupby(by=['cyl', 'mfr'])\n",
    "source = ColumnDataSource(group)\n",
    "\n",
    "p = figure(width=800, height=300, title=\"Mean MPG by # Cylinders and Manufacturer\",\n",
    "           x_range=group, toolbar_location=None, tools=\"\")\n",
    "\n",
    "p.xgrid.grid_line_color = None\n",
    "p.xaxis.axis_label = \"Manufacturer grouped by # Cylinders\"\n",
    "p.xaxis.major_label_orientation = 1.2\n",
    "\n",
    "index_cmap = factor_cmap('cyl_mfr', palette=['#2b83ba', '#abdda4', '#ffffbf', '#fdae61', '#d7191c'], \n",
    "                         factors=sorted(df.cyl.unique()), end=1)\n",
    "\n",
    "p.vbar(x='cyl_mfr', top='mpg_mean', width=1, source=source,\n",
    "       line_color=\"white\", fill_color=index_cmap, \n",
    "       hover_line_color=\"darkgrey\", hover_fill_color=index_cmap)\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=[(\"MPG\", \"@mpg_mean\"), (\"Cyl, Mfr\", \"@cyl_mfr\")]))\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xpj___TiAc-e"
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmxyzoN46cfo",
    "outputId": "e2982391-105b-4b38-c176-2ea9876a2cd1"
   },
   "outputs": [],
   "source": [
    "nb_url=\"http://0.0.0.0:8888\"    # optional, depending on where you are running the notebook \n",
    "show(plot_emb_app, notebook_url=nb_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0Ihqwq96caL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNFofeoO6cdE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuBXAjlo6cXq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBN-9rB86cVB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COkFKxIr6cSI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TWek6W46cO_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QThHMMfU6cLz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRjk0xq76cIj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRC0bu826b-6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c6oNVk25G71"
   },
   "source": [
    "## tl;dr\n",
    "If there is one thing I would like you to take away from this article it is the ability to use the Bokeh library to dynamically visualise, select and **extract** text embeddings of interest **directly from a plot** into a list for further processing with pandas, numpy etc:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pmuKV8O5G73"
   },
   "source": [
    "![Image](https://github.com/morganmcg1/ntentional/blob/master/_notebooks/my_icons/20200629_text_clustering/bokeh4_2020-06-30.gif?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jE8KQFd85G75"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# import IPython\n",
    "# IPython.display.Image(\"https://media.giphy.com/media/QsPZvumxUIn1ZwpIXA/giphy.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucYslbHa5G77"
   },
   "source": [
    "In order for Machine Translation to be useful in the real world, we should should strive to train it on high quality translation data. This is doubly true for lower-resource languages such as Irish, where clean training data is relatively limited. In this article we will try and identify and remove clusters of dirty/noisey samples in our parallalel dataset. The stages we will go through are:\n",
    "\n",
    "- **Generate** embeddings from a pre-trained multi-lingual model, XLM-RoBERTa\n",
    "- **Visualise** these embeddings using a dimensionality technique via UMAP\n",
    "- **Identify** clusters in a sample of the data that seem to be of low translation quality via Bokeh\n",
    "- **Remove** similar samples from the main dataset via nmslib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX78GJIW5G77"
   },
   "source": [
    "> Warning: **Spoiler alert, check out the weird text I found in my Irish-English dataset below 😅**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1oDSznr5G78"
   },
   "source": [
    "**Arabic** ⁉️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LEhZUTA5G79",
    "outputId": "29fb6daf-c900-4f36-f0ed-7f2b7474640d"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "show_similar(samp_embs, fin_selected_indxs, full_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f2RUZos5G7-"
   },
   "source": [
    "**Lighting** 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a61-5LRj5G8A",
    "outputId": "858e4d05-62d0-4754-94b3-b41ebe297adb"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "show_similar(samp_embs, fin_selected_indxs, full_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_RwVVlU5G8A"
   },
   "source": [
    "**Website Footers 📇**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLEqORJi5G8B",
    "outputId": "246fd4f9-a179-45e9-ecc4-9f4726209b09"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "show_similar(samp_embs, fin_selected_indxs, full_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bGIhFOh5G8C"
   },
   "source": [
    "**Clothing and Jewelery** 👑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTu7quV65G8D",
    "outputId": "610810be-ff93-4e2d-b513-a64a22a2e19c"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "show_similar(samp_embs, fin_selected_indxs, full_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeWpfZIg5G8E"
   },
   "source": [
    "## Text Embeddings for Fun and Profit\n",
    "\n",
    "Text embeddings can be incredibly useful for a variety of tasks, a couple of interesting examples include [@er214's post](https://blog.usejournal.com/a-simple-spell-checker-built-from-word-vectors-9f28452b6f26) and [notebook](https://github.com/er214/spellchecker) demonstrating how they used GloVe word embeddings to **create a spell-checker** for their dataset. They also created a [\"pretentiousness\" embedding](https://forums.fast.ai/t/nlp-any-libraries-dictionaries-out-there-for-fixing-common-spelling-errors/16411) to score news outlets 🤣. While these examples used word embeddings, we will generate embeddings for chunks of text, but the concept remains the same.\n",
    "\n",
    "#### Pre-Trained XLM-RoBERTa Model\n",
    "We derive our text embedding by passing a text sample though [XLM-RoBERTa Large](https://arxiv.org/abs/1911.02116) (XLM-R), a multilingual model which should work reasonably well for both Irish and English. For our embeddings we will extract the values in **final** hidden layer. Note that some research done on BERT shows that by concatenating the **final 4 layers** of the model one gets even richer contextual embeddings. In the interest of simplicity we will stick to the final layer only for the moment, although using additional layers would be an interesting area of exploration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6HA8phG5G8G"
   },
   "source": [
    "## Dimension Reduction with UMAP\n",
    "\n",
    "Dimensionality reduction techniques attempt to find the latent features in your data. Uniform Manifold Approximation and Projection or [\"UMAP\"](https://umap-learn.readthedocs.io/en/latest/index.html) is a dimension reduction technique published in [2018 by McInnes and Healy](https://arxiv.org/abs/1802.03426) that can be used for visualisation of high dimensional data, in a similar manner to t-SNE. Its main advantage is that **it is fast** (faster than t-SNE when dealing with large datasets) and also better maintains the global structure of the data. \n",
    "\n",
    "UMAP also has [beautifully well-written](https://umap-learn.readthedocs.io/en/latest/index.html) documentation. To learn more, McInnes gives a [helpful overview](https://www.youtube.com/watch?v=nq6iPZVUxZU) of UMAP at the SciPy 2018 conference and Pair Code has an [excellent explanation](https://pair-code.github.io/understanding-umap/#:~:text=In%20the%20simplest%20sense%2C%20UMAP,behind%20them%20is%20remarkably%20simple) complete with many different visualisations that aid in understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpDxunY55G8G"
   },
   "source": [
    "## Similarity Search\n",
    "In order to calculate the similarity between our embeddings there are a multitude of tools and techniques we can try. In this article we will demonstrate both Sci-kit Learn's `cosine_similarity` as well as `nmslib`'s functionality. See the end of this article for the similarity search resources I found useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0bLgMCV5G8H"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai2.text.all import *\n",
    "from fastai2.basics import *\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "import seaborn as sns\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "\n",
    "from bokeh.layouts import grid\n",
    "from bokeh.models import ColumnDataSource \n",
    "from bokeh.models.widgets.tables import DataTable, TableColumn\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.palettes import d3\n",
    "\n",
    "import umap\n",
    "import umap.plot\n",
    "import nmslib\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8Lb1Y085G8H"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "today = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4QKkedb5G8I"
   },
   "source": [
    "## ParaCrawl Data\n",
    "First lets load our raw [ParaCrawl](https://paracrawl.eu/) data been crawled from the internet and contains a few different types of artifacts including:\n",
    "\n",
    "- Non-latin scripts \n",
    "- Non alphanumeric characters (e.g. '©','³','º')\n",
    "- Text samples that are unlikely to have been translated to Irish by a human, including text from\n",
    "    - Porn sites\n",
    "    - Lighting equipment sites (who knows why?)\n",
    "\n",
    "Lets see how much we can find by turning our samples into embeddings and using dimenstionality reduction to visualise them. For fun, we'll label texts that contain \"sex\", \"lighting\" and cyrillic characters like \"и\", \"з\" or \"л\" see if they get clustered together when we visualise our word embeddings later. We will also lowercae our entire dataset; many of the legal texts here are fully written in uppercase, however right now we care more about the content of the text as opposed to the style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yy9KQ_Wx5G8I",
    "outputId": "67dfdab0-227d-460d-9afb-cd44f2084636",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "path = Path('data/irish/crosslang')\n",
    "ga = pd.read_csv(path/'paracrawl.ga', error_bad_lines=False, sep='\\n\\n', header=None, engine='python')\n",
    "en = pd.read_csv(path/'paracrawl.en', error_bad_lines=False, sep='\\n\\n', header=None, engine='python')\n",
    "df=pd.concat([ga, en], axis=1)\n",
    "df.columns=['ga','en']\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Lowercase\n",
    "df.ga = df.ga.str.lower()\n",
    "df.en = df.en.str.lower()\n",
    "\n",
    "# Label noisey samples\n",
    "df['noise_type'] = 'na'\n",
    "noise_dict = {'sex':'sex','lighting':'lighting','и':'cyrillic', 'л':'cyrillic', 'з':'cyrillic'}\n",
    "for key in noise_dict: df.loc[df.en.str.contains(key), 'noise_type'] = noise_dict[key]\n",
    "\n",
    "#print(f'Dataset has : {len(df)} rows')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN6QHoM65G8J",
    "outputId": "e046f107-1f67-4cc9-84ee-4a617f325eaa"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "print(f'Dataset has : {len(df)} rows')\n",
    "print()\n",
    "print(f'[en] : {df.en.values[50]}')\n",
    "print()\n",
    "print(f'[ga] : {df.ga.values[50]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0N1JKWPB5G8J",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "model = XLMRobertaModel.from_pretrained('xlm-roberta-large')\n",
    "model.cuda()\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "pad_idx=tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mosJtCQJ5G8K"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# ## Create our Dataloader\n",
    "\n",
    "# As part of setting up our dataloader we'll need to define our:\n",
    "# - Tokenizer Transform, which is a wrapper around the HuggingFace tokenizer \n",
    "# - Function to add padding to each sample so that we can batch our samples\n",
    "# - Function to create a padding mask so that our model doesn't attend over the padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pJm981e5G8L"
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "#Tokenizer:\n",
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def encodes(self, x): return torch.tensor((self.tokenizer.encode(x, add_special_tokens=True))) \n",
    "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))\n",
    "\n",
    "# Padding function:\n",
    "def my_pad_input(samples, pad_idx=1, pad_fields=0, pad_first=False):\n",
    "    max_len_l = max([len(s) for s in samples])\n",
    "    def _f(x):\n",
    "        pad =  x.new_zeros(max_len_l-x.shape[0])+pad_idx\n",
    "        x1 = torch.cat([pad, x] if pad_first else [x, pad])\n",
    "        return retain_type(x1, x)\n",
    "    outp=[]\n",
    "    for s in samples: outp.append(_f(s))\n",
    "    return outp   \n",
    "\n",
    "# Padding mask\n",
    "def pad_mask(o):\n",
    "    '0 for masked tokens, else 1'\n",
    "    return (o != pad_idx)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pq6ThMmd5G8M"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# Now we are ready to create our dataloader so that we can quickly iterate through our samples. I'm using fastai here, however you could also use either a PyTorch or Tensorflow dataloader here, the rest of this notebook doesn't have a fastai dependency.\n",
    "\n",
    "# To start, we'll look at a random sample of 40,000 samples, about 5% of our data. Remember we are only retrieving embeddings for our English samples for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODJm7vcA5G8M",
    "outputId": "61070a39-51e0-4575-85d3-631f1d75dc05",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "n_samples=40000\n",
    "lang='en'\n",
    "np.random.seed(42) \n",
    "rand_idxs = np.random.choice(len(df), n_samples, replace=False)\n",
    "samp = df.loc[rand_idxs].copy()\n",
    "samp.reset_index(inplace=True, drop=False)\n",
    "samp.columns = ['main_index', 'ga', 'en', 'noise_type']\n",
    "samp_texts = samp[lang].values\n",
    "all_texts = df[lang].values\n",
    "samp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppaKU-lD5G8N",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "bs,sl = 256,1024\n",
    "\n",
    "def get_dls(texts, bs=256, sl=1024, show=True):\n",
    "    splits = [list(range(len(texts))),list(range(bs))]\n",
    "    tls = TfmdLists(texts, TransformersTokenizer(tokenizer), splits=splits , dl_type=TfmdDL)\n",
    "    dls = tls.dataloaders(bs=bs, seq_len=sl, before_batch=my_pad_input, shuffle_train=False)\n",
    "    if show==True: dls.show_batch(max_n=1)\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5myVko6Y5G8N"
   },
   "source": [
    "## Grab the Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BLgPIhe5G8N"
   },
   "source": [
    "After creating our dataloaders you can see the English sample below, including the special tokens `< s >` and `< /s >` used by XLM-R to denote the start and end of the sequence. You can also see all the padding needed for this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R10Ea2045G8O",
    "outputId": "77e0abb5-2edc-4802-c3be-352993c4b8ba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samp_dls = get_dls(samp_texts, bs, sl, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUE6fV9h5G8O"
   },
   "source": [
    "Now we're ready to retrieve our embeddings from the pretrained multi-lingual model. We do this by simply passing our samples to the model and saving the output activations from the last layer of the model. From this we can generate an embedding of size (40000, 1024) which we can then pass to our dimensionality reduction algorithm. \n",
    "\n",
    "Processing 40k samples with UMAP takes about 5 minutes. After a little testing with the `n_neighbors` and `min_dist` parameters I actually found the defaults work quite well, although its always worth playing around with them and distance metric used for your particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmTtyxYn5G8P",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "def get_text_embeddings(model, dls, emb_nm=None, n_files=1):\n",
    "    '''\n",
    "        Grab logits from model and save embedding file, optionally save across multiple files\n",
    "        n_files : number of files to save the embeddings too, incase they cannot all fit in memory'\n",
    "    ''' \n",
    "    with torch.no_grad():\n",
    "        model.eval() \n",
    "        f_split_interval = int(len(dls.train)/n_files)+1\n",
    "        new_file=False\n",
    "        file_count=0\n",
    "        for i, o in enumerate(progress_bar(dls.train)):\n",
    "            # Extract\n",
    "            mask = pad_mask(o)\n",
    "            _, tmp=model(o, attention_mask=mask)  \n",
    "            \n",
    "            # Store\n",
    "            if i == 0: \n",
    "                embs = tmp\n",
    "            elif new_file:\n",
    "                embs = tmp\n",
    "                new_file=False\n",
    "            else: \n",
    "                embs = torch.cat([embs,tmp])\n",
    "            \n",
    "            # Save\n",
    "            if n_files>1:\n",
    "                if (i > 0) & (i % (f_split_interval) == 0): \n",
    "                    torch.save(embs, emb_nm + f'_{file_count}')\n",
    "                    print(f'Embedding file {file_count} saved')\n",
    "                    file_count+=1\n",
    "                    del embs\n",
    "                    gc.collect()\n",
    "                    new_file=True\n",
    "                elif i == len(dls.train)-1: \n",
    "                    torch.save(embs, emb_nm + f'_{file_count}')\n",
    "                    new_file=True\n",
    "                    print(f'Embedding file {file_count} saved')\n",
    "            elif (n_files==1) & (i == len(dls.train)-1):\n",
    "                torch.save(embs, emb_nm)\n",
    "                \n",
    "        # RELOAD SAVED EMBEDDING FILES IF n_files>1\n",
    "        if n_files>1:\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            for j in range(0,n_files):\n",
    "                if j==0: embs = torch.load(emb_nm + f'_{int(j):.0f}')\n",
    "                else:\n",
    "                    tmp = torch.load(emb_nm + f'_{int(j):.0f}')\n",
    "                    embs = torch.cat([embs,tmp])\n",
    "            torch.save(embs, emb_nm+'_full')\n",
    "\n",
    "        print(f'Embedding tensor(s) saved here: \"{emb_nm}\", full tensor size is : {embs.size()}')\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opY6jQn55G8P",
    "outputId": "5f326627-2c2e-462d-fccb-c09ac3546b2c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "samp_emb_nm = f'models/xlm-r_large_embs_en_samp_{today}'\n",
    "samp_embs = get_text_embeddings(model, samp_dls, emb_nm=samp_emb_nm, n_files=1)\n",
    "samp_embs=samp_embs.to('cpu')\n",
    "\n",
    "#samp_embs = torch.load('models/xlm-r_large_embs_en_samp_2020-06-29').to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5ybyHGj5G8Q"
   },
   "source": [
    "**Scaling your Embeddings**\n",
    "\n",
    "One thing to consider might be whether you should scale your embeddings. The embeddings used here had mean 0 and standard deviation of 0.5, normalising them to (0,1) didn't seem to have much of an impact on the UMAP visualisation so it is not done here. Scikit-Learn has a [wide variety of scaling functions](https://scikit-learn.org/stable/modules/preprocessing.html) if you do need to scale your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24jmq3n45G8R"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# std=samp_embs.std()\n",
    "# m=samp_embs.mean()\n",
    "\n",
    "# scaled_embs = (samp_embs - m)/std\n",
    "# scaled_embs.mean(), scaled_embs.std()\n",
    "\n",
    "# scaled_mapper = umap.UMAP(random_state=42).fit(scaled_embs.cpu().numpy())\n",
    "# scaled_p = umap.plot.interactive(scaled_mapper, hover_data=samp, point_size=2, labels=samp.noise_type)\n",
    "# output_notebook()\n",
    "# show(scaled_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKNcbJxX5G8R"
   },
   "source": [
    "## Remove similar samples from the entire dataset\n",
    "We will remove items by:\n",
    "\n",
    "1. Visually **identifying** similar noisey embeddings\n",
    "\n",
    "2. Taking the **average of these embeddings**\n",
    "\n",
    "3. Calculating a **distance** between this average embedding and all of the embeddings in our entire dataset\n",
    "\n",
    "3. Removing embeddings that are within a certain **distance** to the average embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeE8gSO45G8R"
   },
   "source": [
    "## Visualise: Dimensionality Reduction with UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaUyCM4r5G8S"
   },
   "source": [
    "First we reduce the embedding space down from 1024 to 2 dimensions using **UMAP**, then we plot the embedding using the **Bokeh** visualisation library, which enables us to dynamically select different regions of interest, enabling us to interactively explore our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNZJkmpf5G8S"
   },
   "outputs": [],
   "source": [
    "samp_mapper = umap.UMAP(random_state=42).fit(samp_embs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2sC0LSW5G8T",
    "outputId": "f34974e7-eeda-4777-c0a5-f6e6b8691a2d"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "output_notebook()\n",
    "\n",
    "def plot_emb_app(doc):\n",
    "    # CONSTANTS\n",
    "    l = len(samp) \n",
    "    \n",
    "    lang1, lang2, label_col = 'en', 'ga', 'noise_type'\n",
    "    plot_width,plot_height = 525,525\n",
    "    \n",
    "    # DATA\n",
    "    data = pd.DataFrame(samp_mapper.embedding_[:l], columns=(\"x\", \"y\"))\n",
    "    df = samp[[lang1,lang2,'main_index', label_col]].iloc[:l,:]\n",
    "    hover_data = df[[lang1,lang2,label_col,'main_index']]\n",
    "    data[label_col] = df[label_col]\n",
    "    \n",
    "    # SET LABELS\n",
    "    labels = samp[label_col]\n",
    "     \n",
    "    # TOOLTIP\n",
    "    tooltip_dict = {}\n",
    "    for col_name in hover_data:\n",
    "        data[col_name] = hover_data[col_name]\n",
    "        tooltip_dict[col_name] = \"@\" + col_name\n",
    "    tooltips = list(tooltip_dict.items())\n",
    "    \n",
    "    # COLOR BY LABEL\n",
    "    palette = d3['Category10'][len(df[label_col].unique())]\n",
    "    colormap = {x:palette[i] for i,x in enumerate(df[label_col].unique())}\n",
    "    data['colors'] = [colormap[x] for x in df[label_col]]\n",
    "    \n",
    "    # PLOT1\n",
    "    s1 = ColumnDataSource(data=data)\n",
    "    p1 = figure(\n",
    "        tooltips=tooltips,\n",
    "        plot_width=plot_width,\n",
    "        plot_height=plot_height,\n",
    "        tools=['lasso_select','pan','wheel_zoom','box_zoom','reset','save'],\n",
    "        active_drag=\"lasso_select\",\n",
    "        title=\"Select points with Lasso tool\",\n",
    "    )\n",
    "    p1.title.vertical_align = 'top'\n",
    "    \n",
    "    # CREATE CIRCLE 1\n",
    "    c1=p1.circle(\"x\", \"y\", source=s1, alpha=0.5, color='colors', size=2)\n",
    "    ds1 = c1.data_source\n",
    "    \n",
    "    # PLOT2\n",
    "    s2 = ColumnDataSource(data=dict(x=[], y=[], idx=[]))\n",
    "    p2 = figure(\n",
    "        plot_width=int(plot_width/2),\n",
    "        plot_height=int(plot_height/2),\n",
    "        x_range=(min(data['x'])-1, max(data['x'])+1),\n",
    "        y_range=(min(data['y'])-1, max(data['y'])+1),\n",
    "        title=\"Selected Points from Main Plot\",\n",
    "    )\n",
    "    p2.title.vertical_align = 'top'\n",
    "    c2 = p2.circle(\"x\", \"y\", source=s2, alpha=0.6, color=\"firebrick\", size=3)\n",
    "    ds2 = c2.data_source\n",
    "    \n",
    "    # SELECT CALLBACK\n",
    "    def select_callback(attrname, old, new):\n",
    "        global selected_indxs        \n",
    "        d1 = ds1.data\n",
    "        d2 = dict()\n",
    "        selected_indxs = new\n",
    "        xs,ys,t1,t2,idxs,lab1 = [],[],[],[],[],[]\n",
    "        \n",
    "        for i in range(len(selected_indxs)):\n",
    "            xs.append(d1['x'][selected_indxs[i]])\n",
    "            ys.append(d1['y'][selected_indxs[i]])\n",
    "            lab1.append(d1[label_col][selected_indxs[i]])\n",
    "            t1.append(d1[lang1][selected_indxs[i]])\n",
    "            t2.append(d1[lang2][selected_indxs[i]])\n",
    "        d2['x'], d2['y'], d2['idx'], d2[lang1], d2[lang2], d2[label_col] = xs, ys, selected_indxs, t1, t2, lab1\n",
    "        ds2.data = d2 # BEST PRACTICE --- update .data in one step with a new dict\n",
    "    \n",
    "    s1.selected.on_change('indices', select_callback)\n",
    "\n",
    "    # TABLE OF SELECTED POINTS\n",
    "    columns = [\n",
    "        TableColumn(field=\"idx\", title=\"data idx\", width=15),\n",
    "        TableColumn(field=lang1, title=lang1),\n",
    "        TableColumn(field=lang2, title=lang2),\n",
    "        TableColumn(field=label_col, title=label_col, width=50),\n",
    "        TableColumn(field=\"x\", title=\"X axis\", width=40),\n",
    "        TableColumn(field=\"y\", title=\"Y axis\", width=40),\n",
    "    ]\n",
    "    table = DataTable(\n",
    "        source=s2,\n",
    "        columns=columns,\n",
    "        width=900,\n",
    "        height=400,\n",
    "        sortable=True,\n",
    "        selectable=True,\n",
    "        editable=True,\n",
    "    )\n",
    "    \n",
    "    doc.add_root(grid([p1, p2, table], ncols=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Q2re73Q5G8U",
    "outputId": "c9ff5101-5a0b-4855-eae3-bbcb1347b905"
   },
   "outputs": [],
   "source": [
    "nb_url=\"http://0.0.0.0:8888\"    # optional, depending on where you are running the notebook \n",
    "show(plot_emb_app, notebook_url=nb_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVAoY0CH5G8V",
    "outputId": "c92c1733-b044-4c15-8b44-15d7aa80ece7"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "![Image](my_icons/20200629_text_clustering/bokeh_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnigMYAB5G8W"
   },
   "source": [
    "Below we can see how we can explore how our data has been clustered together and more interestingly **select and extract** datapoints of interest **directly from the plot** into a list for further processing with pandas, numpy etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7_F65Xj5G8W",
    "outputId": "cc6b6c35-6e97-4198-8015-e3d11161104b"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "from IPython.display import Image\n",
    "Image(filename='my_icons/20200629_text_clustering/bokeh_viz.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyWIaXZk5G8W"
   },
   "source": [
    "## Identify clusters in a sample of the data that seem to be of low translation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR0BK5p05G8W"
   },
   "source": [
    "Can we identify suspect looking clusters of data? Yes! \n",
    "\n",
    "- We can see the orange \"islands\" that we have labelled all contain text related to \"lighting\", \"LED\", \"Lamps\" etc, some of which are not even in English.\n",
    "\n",
    "- The sparse cluster of green blue points and their neighbours are also full of language related to pornography\n",
    "\n",
    "- We don't see so many Cryllic points, however this was also the least common of our labels, comprising only (0.07% of our labels)\n",
    "\n",
    "Other \"islands\" that can identified by hovering over them include:\n",
    "- Arabic texts\n",
    "- Website footers\n",
    "- Text from jewellery sites (e.g. Pandora) and also clothing sites\n",
    "\n",
    "#### It Worked!\n",
    "Below you can see additional Arabic texts in our main dataset we have discovered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6vaPzE95G8Y",
    "outputId": "ff5a56fc-70a5-412e-89af-da4792a7afb1"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "# PULL SELECTED DATA FROM PLOT INTO DATAFRAME\n",
    "fin_selected_indxs = selected_indxs\n",
    "df.loc[samp.loc[fin_selected_indxs]['main_index']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNNIs_K35G8Y"
   },
   "source": [
    "I am fairly confident that none of the text in some these islands contain valuable translations and are likely the result of automated translations of suspect quality that we would like to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9RoiyXw5G8Z"
   },
   "source": [
    "## Remove similar samples from the full dataset\n",
    "To remove similar samples we will calculate a distance metric between each of our selected embeddings and each of the embeddings in the full dataset. Alternativly we could also calculate the \"average embedding\" of all of our selected datapoints, and then calculate the distance between this average and each of the embeddings in the main dataset. \n",
    "\n",
    "From experimentation I found that the first option is more effective at identifying more of the noisy data we are looking for. In addtion, because our nms algorithm is super fast at retrieval there isn't any significant overhead to this approach over using the average embedding.\n",
    "\n",
    "First of all, we'll need to generate embeddings for all of our 780k text samples. This might take a little while depending on the size of your dataset so kick off the extraction and grab a coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_I4XYTGg5G8a"
   },
   "outputs": [],
   "source": [
    "#hide \n",
    "# CALCULATE AVERAGE OF SELECTED EMBEDDINGS\n",
    "def calc_avg_emg(samp_embs, fin_selected_indxs):\n",
    "    selected_embs = samp_embs[fin_selected_indxs]\n",
    "    avg_emb = torch.zeros(selected_embs[0].size())\n",
    "    for e in selected_embs: \n",
    "        avg_emb += e\n",
    "    avg_emb /= len(selected_embs)\n",
    "    avg_np=np.expand_dims(avg_emb.numpy(), axis=0)\n",
    "    return avg_np\n",
    "\n",
    "def avg_query(full_index, avg_np, k=100000):\n",
    "    ids, distances = full_index.knnQuery(avg_np, k=k)\n",
    "    avg_df = pd.DataFrame({'inds':ids,'dists':distances})\n",
    "    # drop the query indices\n",
    "    avg_df=avg_df.loc[~avg_df.inds.isin(fin_selected_main_indxs)]\n",
    "    # drop duplicate indices (distances will be biassed towards early query values)\n",
    "    avg_df.drop_duplicates(subset='inds', inplace=True, ignore_index=True)\n",
    "    return avg_df\n",
    "\n",
    "def show_similar(samp_embs, fin_selected_indxs, full_index):\n",
    "    avg_np=calc_avg_emg(samp_embs, fin_selected_indxs)\n",
    "    avg_df=avg_query(full_index, avg_np, k=100000)\n",
    "    print(f'{len(avg_df)} ids returned from query')\n",
    "    return df.loc[avg_df.inds.values].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miGVicvJ5G8a",
    "outputId": "a4dc54a5-e7b4-4df1-c62d-a4541be00685"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# full_dls = get_dls(all_texts, bs, show=False)\n",
    "# full_emb_nm = f'models/xlm-r_large_embs_en_full_{today}'\n",
    "# full_emb = get_text_embeddings(model, full_dls, emb_nm=full_emb_nm, n_files=6).to('cpu')\n",
    "# full_emb.size()\n",
    "\n",
    "full_emb = torch.load('models/xlm-r_large_embs_en_full_2020-06-19_full', 'cpu')\n",
    "full_emb.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftFLsAHW5G8b"
   },
   "source": [
    "## Sci-kit Learn's cosine_similarity\n",
    "\n",
    "By calculating the average embedding of our selected datapoints we can calculate the cosine similarity between it and all of the other embeddings in our full dataset, providing decent results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OBR0Qyk5G8b",
    "outputId": "03d17f0a-8177-4df8-cf3d-33cd85468825",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "thresh=0.9995\n",
    "avg_emb=calc_avg_emg(samp_embs, fin_selected_indxs)\n",
    "cs = cosine_similarity(avg_emb, full_emb.numpy())\n",
    "\n",
    "cs_df = pd.DataFrame({'score':cs[0]})\n",
    "cs_df['main_index'] = cs_df.index.values\n",
    "cs_df=cs_df.loc[~cs_df.main_index.isin(fin_selected_main_indxs)]    # Remove selected indices from similarity results\n",
    "cs_df.sort_values('score',inplace=True)\n",
    "\n",
    "idx = cs_df.loc[cs_df.score>thresh].index.values\n",
    "print(f'{len(idx)} rows with a similarity score greater than : {thresh}')\n",
    "df.loc[idx].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuIMMAX95G8c",
    "outputId": "d84098c1-54a8-4ede-d29c-aea2b5c05567"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "sns.distplot(cs_df.score, kde=False)\n",
    "plt.title('Cosine similarity between the average selection embedding and the full datset');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbh1JtqU5G8d"
   },
   "source": [
    "**We could stop at this point and continue to loop through selecting new datapoints of interest and removing them from our full dataset, but for fun lets look at another way to do similarity search, using nmslib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMiVBVwX5G8e"
   },
   "source": [
    "## Creating a nmslib Index\n",
    "> Note: *As an alternative to Sci-kit Learn's cosine similarity function we can also use nmslib to calculate our similarity. Note this method is slower given our needs in this example, but its always fun to work with a new technology* 😀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elnvGelQ5G8e"
   },
   "source": [
    "Once we have all of our embeddings we can create our index with nmslib. This is the most time-consuming part of this process, it took 47minutes for the index to be created in this example, although there are other nmslib settings thatn can reduce this to ~15minutes.\n",
    "\n",
    "We create an index for our entire dataset **only once**.  We create an index for the entire dataset because we will likely have multiple queries we do not want to re-create a new index each time as it will really slow down how fast we can identify and remove low quality data.\n",
    "\n",
    "> Note: (This index will now include our sample datapoints, which are the same datapoints that we use in our query to the index. Therefore we will have to exclude these sample datapoints from our query result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbxFN5Lg5G8f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "full_index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "full_index.addDataPointBatch(full_emb.numpy())\n",
    "\n",
    "# CREATE\n",
    "# Creating an index for our 784k embeddings takes 47 minutes. we can trade off the quality of our index for a faster creation time if we'd like.\n",
    "# 'post','M','efConstruction' parameter explanations here: https://github.com/nmslib/nmslib/blob/master/manual/methods.md\n",
    "full_index.createIndex({'post': 2, 'M':80, 'efConstruction':1000}, print_progress=True)\n",
    "\n",
    "# SAVE\n",
    "full_index_nm='models/full_index.bin'\n",
    "full_index.saveIndex(full_index_nm, save_data=True)\n",
    "\n",
    "# LOAD\n",
    "# To load the index again\n",
    "# index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "# index.loadIndex(full_index_nm, load_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpxpZdWo5G8f"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "#NMS resources\n",
    "# Code from https://github.com/benfred/bens-blog-code/blob/master/approximate_als/test_batch_queries.py\n",
    "#         Associated blog post: http://www.benfrederickson.com/approximate-nearest-neighbours-for-recommender-systems/\n",
    "#         nmslib parameter details here: https://github.com/nmslib/nmslib/blob/master/manual/methods.md\n",
    "\n",
    "# Sample Class:\n",
    "# class NMSLibIndex(object):\n",
    "#     ''' \n",
    "#         Taken from https://github.com/benfred/bens-blog-code/blob/master/approximate_als/test_batch_queries.py\n",
    "#         Associated blog post: http://www.benfrederickson.com/approximate-nearest-neighbours-for-recommender-systems/\n",
    "#         nmslib parameter details here: https://github.com/nmslib/nmslib/blob/master/manual/methods.md\n",
    "#     '''\n",
    "#     def __init__(self, method, indexparams, queryparams):\n",
    "#         self.method = method\n",
    "#         self.indexparams = indexparams\n",
    "#         self.queryparams = queryparams\n",
    "\n",
    "#     def fit(self, data):\n",
    "#         self.index = nmslib.init(method=self.method, space='cosinesimil')\n",
    "#         self.index.addDataPointBatch(data)\n",
    "#         self.index.createIndex(self.indexparams, print_progress=True)\n",
    "\n",
    "#     def query_batch(self, queries, n=10):\n",
    "#         return self.index.knnQueryBatch(queries, n)\n",
    "\n",
    "# indices = {\n",
    "#         'hnsw(post=0)': NMSLibIndex(method='hnsw',\n",
    "#                                    indexparams=[\n",
    "#                                        'M=32', 'post=0', 'efConstruction=800'],\n",
    "#                                    queryparams=['ef=90']),\n",
    "#         'hnsw(M=50post=2ef1500)': NMSLibIndex(method='hnsw',\n",
    "#                                    indexparams=[\n",
    "#                                        'M=50', 'post=2', 'efConstruction=1500'],\n",
    "#                                    queryparams=['ef=500']),\n",
    "#         'hnsw(post=2)': NMSLibIndex(method='hnsw',\n",
    "#                                    indexparams=[\n",
    "#                                        'M=32', 'post=2', 'efConstruction=800'],\n",
    "#                                    queryparams=['ef=50'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8zZodBN5G8g"
   },
   "source": [
    "## Querying the Index\n",
    "Below are the sampled results from querying the full dataset, found through similarity search with an average embedding of the selection from the bokeh plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEvG7G3y5G8g"
   },
   "source": [
    "**Arabic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMNVZdqm5G8g",
    "outputId": "1709ffc1-64bd-4571-b8cd-bce727741694"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "show_similar(samp_embs, fin_selected_indxs, full_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ytubz3pF5G8h"
   },
   "source": [
    "**Lighting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlyUbKqt5G8h",
    "outputId": "db3466cc-a7ba-4020-ff1b-6926948f4c81"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "show_similar(samp_embs, fin_selected_indxs, full_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L95TtVnI5G8i"
   },
   "source": [
    "**Website Footer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIZiYvtH5G8i",
    "outputId": "870d4ebe-113a-4676-8fe2-c035c714c663"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "show_similar(samp_embs, fin_selected_indxs, full_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R90HblbB5G8i"
   },
   "source": [
    "**Clothing and Jewelery**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bECfl9II5G8j",
    "outputId": "438ca82c-c031-47dc-8bea-06075a84436d"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "show_similar(samp_embs, fin_selected_indxs, full_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_rv7VSu5G8k"
   },
   "source": [
    "## Removal\n",
    "If we like we can limit our removal to only very similar embeddings in the full dataset by only selecting the datapoints in the full dataset that are sufficiently close (lower score) to the average embedding, plotting the score distribution can help us decide on a suitable threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVEweSPT5G8k",
    "outputId": "061ad564-76eb-46b0-cc0d-e597c22a3059"
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "avg_np=calc_avg_emg(samp_embs, fin_selected_indxs)\n",
    "avg_df=avg_query(full_index, avg_np, k=100000)\n",
    "sns.distplot(avg_df.dists, kde=False)\n",
    "plt.title('Cosine similarity between the average selection embedding and the full datset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnVyQzOh5G8l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# # [ALTERNATIVE] Find similar embeddings for every individual selected datapoint\n",
    "# #avg_np=np.expand_dims(avg_emb.numpy(), axis=0)\n",
    "# selected_embs = samp_embs[fin_selected_indxs];selected_embs.size()\n",
    "\n",
    "# # Translate our selected indices from the sample dataset into indices in the full dataset\n",
    "# fin_selected_main_indxs = df.loc[samp.loc[fin_selected_indxs]['main_index']].index.values.tolist()\n",
    "\n",
    "# full_index.setQueryTimeParams({'efSearch': 1000})\n",
    "\n",
    "# neighbours_dict={}\n",
    "# for k in [10,100,1000,10000]:\n",
    "#     neighbours = full_index.knnQueryBatch(selected_embs.numpy(), k=k)   # returns for each query: (indices, distances)\n",
    "\n",
    "#     inds_np_ls = [n[0] for n in neighbours]\n",
    "#     dists_np_ls = [n[1] for n in neighbours]\n",
    "#     tmp_inds = np.unique(np.concatenate(inds_np_ls).ravel())\n",
    "#     tmp_dists = np.concatenate(dists_np_ls).ravel()\n",
    "    \n",
    "#     tmp_df = pd.DataFrame({'inds':np.concatenate(inds_np_ls).ravel(),\n",
    "#                           'dists':np.concatenate(dists_np_ls).ravel()})\n",
    "#     # drop the query indices\n",
    "#     tmp_df=tmp_df.loc[~tmp_df.inds.isin(fin_selected_main_indxs)]\n",
    "#     # drop duplicate indices (distances will be biassed towards early query values)\n",
    "#     tmp_df.drop_duplicates(subset='inds', inplace=True, ignore_index=True)\n",
    "\n",
    "# #     # Remove the query indices\n",
    "# #     sorter = np.argsort(tmp_inds)\n",
    "# #     to_remove=sorter[np.searchsorted(tmp_inds, fin_selected_main_indxs, sorter=sorter)]   # indices to remove\n",
    "# #     fin_inds = np.delete(tmp_inds, to_remove)\n",
    "    \n",
    "#     neighbours_dict[f'k{k}_inds'] = tmp_df.inds.values #np.delete(tmp_inds, to_remove)\n",
    "#     neighbours_dict[f'k{k}_dists'] = tmp_df.dists.values #np.delete(tmp_dists, to_remove)\n",
    "#     print(f'{len(tmp_df.inds.values)} unique & similar indices returned for k = {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIr7CfF65G8m"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# #hide_input\n",
    "# # for k 10,100,1000\n",
    "# colors=['skyblue', 'purple', 'black', 'teal']\n",
    "# for i,k in enumerate([10,100,1000,10000]):\n",
    "#     sns.distplot(neighbours_dict[f'k{k}_dists'] , color=colors[i], label=f'k={k}', kde=False)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3KWuOKS5G8m"
   },
   "source": [
    "## Summary\n",
    "Hopefully this gives you a sense of how you can explore and clean up large, noisy text datasets. You can open this notebook on github and test it for your own text dataset. \n",
    "\n",
    "As always, I would love to hear your feedback, what could have been written better or clearer, you can find me on twitter: [@mcgenergy](www.twitter.com/mcgenergy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2c6oNVk25G71"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
